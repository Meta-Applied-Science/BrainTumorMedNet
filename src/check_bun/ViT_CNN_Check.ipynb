{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdb60cf1",
   "metadata": {},
   "source": [
    "# **SETUP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b059311c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma012/AlexServer/phase2/btc/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/ma012/AlexServer/phase2/src\")\n",
    "sys.path.append(\"/home/ma012/AlexServer/phase2/src/utils\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "import timm\n",
    "import torchvision.models as models\n",
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "from model_utils import load_vit_model_config,load_cnn_model_config, set_seeds\n",
    "from data_loader import create_dataloaders,create_dataloaders_2\n",
    "\n",
    "from skimage.segmentation import mark_boundaries\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "sys.path.append(os.path.abspath(os.path.join(current_dir, \"..\")))\n",
    "\n",
    "from going_modular.going_modular.engine import trainVal, test_step, load_model_checkpoint, ensemble_test_step,ensemble_inference_step,my_ensemble_test_step,max_each_ensemble,inference_step\n",
    "from helper_functions import plot_loss_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92228999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7955a5a",
   "metadata": {},
   "source": [
    "## General Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76ef85ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "NUM_WORKERS = 0 # os.cpu_count() \n",
    "\n",
    "BATCH_SIZE_64 = 64\n",
    "BATCH_SIZE_32 = 32\n",
    "BATCH_SIZE_16 = 16\n",
    "BATCH_SIZE_8 = 8\n",
    "\n",
    "EPOCHS_25 = 25\n",
    "EPOCHS_20 = 20\n",
    "EPOCHS_15 = 15\n",
    "EPOCHS_10 = 10\n",
    "\n",
    "# LR_0_00001 = 0.00001\n",
    "LR_0_0001 = 0.0001\n",
    "LR_0_1 = 0.1\n",
    "\n",
    "LR_0_00005 = 0.00005\n",
    "LR_0_05 = 0.05\n",
    "\n",
    "LOG_CHECK = \"/home/ma012/AlexServer/phase2/log/xai_check.txt\"\n",
    "\n",
    "DATASET_DIR = \"/home/ma012/AlexServer/Dataset\"\n",
    "DATASET_KAGGLE_DIR = \"/home/ma012/AlexServer/KaggleDataset/reorganized\"\n",
    "DATASET_BT_LARGE_4C = \"/home/ma012/AlexServer/phase2/dataset/BT_Large_4c\"\n",
    "\n",
    "NUM_CLASSES = 4 #3\n",
    "CLASS_NAMES = ['glioma_tumor', 'meningioma_tumor','no_tumor','pituitary_tumor']\n",
    "\n",
    "TRAIN_RATIO = 0.7\n",
    "VAL_RATIO = 0.1\n",
    "TEST_RATIO = 0.2\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6610986c",
   "metadata": {},
   "source": [
    "# **ViT training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bb0775",
   "metadata": {},
   "source": [
    "## ViT Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c490eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LOG_32 = \"/home/ma012/AlexServer/log/batch32.txt\"\n",
    "LOG_1K = \"/home/ma012/AlexServer/log/train_1k.txt\"\n",
    "LOG_21k = \"/home/ma012/AlexServer/log/train_21k.txt\"\n",
    "LOG_21k_1k = \"/home/ma012/AlexServer/log/train_21k_1k.txt\"\n",
    "\n",
    "# SAVE_PATH = \"/home/ma012/AlexServer/log/ViT_B_16/RMSprop/case3.pth\"\n",
    "# SAVE_PATH = \"/home/ma012/AlexServer/log/ViT_L_16/RMSprop/new_21k_case3.pth\"\n",
    "# SAVE_PATH = \"/home/ma012/AlexServer/log/ViT_B_32/adam/21k_case33.pth\"\n",
    "SAVE_PATH = \"/home/ma012/AlexServer/log/ViT_L_16/Adadelta/21k_case33.pth\"\n",
    "# SAVE_PATH = \"/home/ma012/AlexServer/log/ViT_L_32/adam/21k_case1.pth\"\n",
    "\n",
    "# SAVE_PATH = \"/home/ma012/AlexServer/log/ViT_L_16/adam/new_21k_case2.pth\"\n",
    "# SAVE_PATH = \"/home/ma012/AlexServer/log/ViT_L_16/Adadelta/new_21k_case3.pth\"\n",
    "\n",
    "\n",
    "# SAVE_PATH = \"/home/ma012/AlexServer/log/case384/checkpoint/checkpoint_case2_augred_in1k.pth\"\n",
    "# SAVE_PATH = \"/home/ma012/AlexServer/log/case384/checkpoint/checkpoint_case4_orig_in21k_in1k.pth\"\n",
    "\n",
    "\n",
    "# MODEL_CFG = \"torchvision.ViT_B_16_Weights.IMAGENET1K_V1\"\n",
    "# MODEL_CFG = \"torchvision.ViT_B_32_Weights.IMAGENET1K_V1\"\n",
    "# MODEL_CFG = \"torchvision.ViT_L_16_Weights.IMAGENET1K_V1\"\n",
    "# MODEL_CFG = \"torchvision.ViT_L_32_Weights.IMAGENET1K_V1\"\n",
    "\n",
    "# ======== Augment 21k ======\n",
    "# MODEL_CFG = \"timm.vit_base_patch16_224.augreg_in21k.PRETRAINED\" \n",
    "# MODEL_CFG = \"timm.vit_base_patch32_224.augreg_in21k.PRETRAINED\"\n",
    "MODEL_CFG = \"timm.vit_large_patch16_224.augreg_in21k.PRETRAINED\"\n",
    "# MODEL_CFG = \"timm.vit_large_patch32_224.orig_in21k.PRETRAINED\" \n",
    "\n",
    "# === Augment 21k and fine-tuned 1k\n",
    "# MODEL_CFG = \"timm.vit_base_patch16_224.augreg2_in21k_ft_in1k.PRETRAINED\" \n",
    "# MODEL_CFG = \"timm.vit_base_patch32_224.augreg_in21k_ft_in1k.PRETRAINED\" \n",
    "# MODEL_CFG = \"timm.vit_large_patch16_224.augreg_in21k_ft_in1k.PRETRAINED\" \n",
    "# MODEL_CFG = \"timm.vit_large_patch32_224.augreg_in21k_ft_in1k.PRETRAINED\" # error\n",
    "\n",
    "# CASE 384x384\n",
    "# MODEL_CFG = \"timm.vit_base_patch16_384.augreg_in1k.PRETRAINED\"\n",
    "# MODEL_CFG = \"timm.vit_base_patch32_384.augreg_in1k.PRETRAINED\"\n",
    "\n",
    "# MODEL_CFG = \"timm.vit_base_patch16_384.orig_in21k_ft_in1k\"\n",
    "# MODEL_CFG = \"timm.vit_base_patch32_384.augreg_in21k_ft_in1k\"\n",
    "# MODEL_CFG = \"timm.vit_large_patch16_384.augreg_in21k_ft_in1k\"\n",
    "# MODEL_CFG = \"timm.vit_large_patch32_384.orig_in21k_ft_in1k\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c142e5",
   "metadata": {},
   "source": [
    "## ViT RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a77728f",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(SEED)\n",
    "model,model_transforms = load_vit_model_config(MODEL_CFG, NUM_CLASSES)\n",
    "                       \n",
    "# case 224x224\n",
    "custom_transforms = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    model_transforms\n",
    "])\n",
    "\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=LR_0_00005)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=LR_0_00005)\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=LR_0_05)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "dataset_dir = \"/home/ma012/AlexServer/Dataset\"\n",
    "\n",
    "train_loader, val_loader, test_loader = create_dataloaders(\n",
    "    dataset_dir=dataset_dir,\n",
    "    transform=custom_transforms,\n",
    "    batch_size= BATCH_SIZE_16,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    train_ratio=TRAIN_RATIO,\n",
    "    val_ratio=VAL_RATIO,\n",
    "    test_ratio=TEST_RATIO,\n",
    "    seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289b462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_loader))\n",
    "print(len(val_loader))\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0548b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainVal(model, train_loader, val_loader, optimizer, loss_fn,\n",
    "                    EPOCHS_15, DEVICE, SAVE_PATH,LOG_32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a656f598",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(results, type=\"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2d6f39",
   "metadata": {},
   "source": [
    "# **ViT Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8932262",
   "metadata": {},
   "outputs": [],
   "source": [
    "ViT_CHECKPOINT = [\n",
    "    # Figshare Au/home/ma012/AlexServer/phase2/log/drive/Figshare Augmentation/L16_21K_case3.pthgmentation\n",
    "    [\"/home/ma012/AlexServer/phase2/log/drive/Figshare Augmentation/L16_21K_case3.pth\", \"timm.vit_large_patch16_224.augreg_in21k.PRETRAINED\"],\n",
    "\n",
    "    # BT-4C\n",
    "    [\"/home/ma012/AlexServer/phase2/log/drive/ BT-4C/L16_1K_case3.pth\", \"torchvision.ViT_L_16_Weights.IMAGENET1K_V1\"], \n",
    "\n",
    "    # Hugging Face\n",
    "    [\"/home/ma012/AlexServer/phase2/log/drive/Hugging Face/L16_1K_case3.pth\", \"torchvision.ViT_L_16_Weights.IMAGENET1K_V1\"], \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e363073",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(SEED)\n",
    "model,model_transforms = load_vit_model_config(ViT_CHECKPOINT[1][1], NUM_CLASSES)\n",
    "\n",
    "CUSTOM_TRANSFORMS = transforms.Compose([\n",
    "    # transforms.Grayscale(num_output_channels=3),\n",
    "    model_transforms\n",
    "])\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader, val_loader, test_loader = create_dataloaders_2(\n",
    "    dataset_dir=DATASET_BT_LARGE_4C,\n",
    "    transform=CUSTOM_TRANSFORMS,\n",
    "    batch_size= BATCH_SIZE_32,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    # train_ratio=TRAIN_RATIO,\n",
    "    val_ratio=VAL_RATIO,\n",
    "    # test_ratio=TEST_RATIO,\n",
    "    seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516b5f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT original head\n"
     ]
    }
   ],
   "source": [
    "model_name = ['ViT B-16','ViT B-32', 'ViT L-16', 'ViT L-32']\n",
    "\n",
    "for idx, (cp_path, model_cfg) in enumerate(BEST_CHECKPOINT):\n",
    "    with open(\"/home/ma012/AlexServer/log/test_results.txt\", \"a\", encoding=\"utf-8\") as file:\n",
    "        file.write(f\"\\n ======================   {model_name[idx]}  ==================================\\n\")\n",
    "\n",
    "    model,model_transforms = load_vit_model_config(model_cfg, NUM_CLASSES)\n",
    "    model = load_model_checkpoint(model, cp_path, DEVICE)\n",
    "\n",
    "    test_results = test_step(model, test_loader, loss_fn, DEVICE,LOG_CHECK)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b38c0b",
   "metadata": {},
   "source": [
    "## Specific ViT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce788abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model,model_transforms = load_vit_model_config(ViT_CHECKPOINT[1][1], NUM_CLASSES)\n",
    "model = load_model_checkpoint(model, ViT_CHECKPOINT[1][0], DEVICE)\n",
    "\n",
    "test_results = test_step(model, test_loader, loss_fn, DEVICE,LOG_CHECK)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba5970f",
   "metadata": {},
   "source": [
    "# **ENSEMBLE-TEST**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c283ede",
   "metadata": {},
   "source": [
    "## ViT Ensemble Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce5fa87",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc352d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_CHECKPOINT = [\n",
    "    [\"/home/ma012/AlexServer/log/vit_best/B16_21K_case3.pth\", \"timm.vit_base_patch16_224.augreg_in21k.PRETRAINED\"],\n",
    " \n",
    "    [\"/home/ma012/AlexServer/log/vit_best/B32_21K_case2.pth\", \"timm.vit_base_patch32_224.augreg_in21k.PRETRAINED\"], \n",
    "\n",
    "    [\"/home/ma012/AlexServer/log/vit_best/L16_21K_case3.pth\", \"timm.vit_large_patch16_224.augreg_in21k.PRETRAINED\"], \n",
    " \n",
    "    [\"/home/ma012/AlexServer/log/vit_best/L32_21K_case1.pth\",  \"timm.vit_large_patch32_224.orig_in21k.PRETRAINED\" ] \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8a57111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    Resize(size=248, interpolation=bicubic, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    MaybeToTensor()\n",
      "    Normalize(mean=tensor([0.5000, 0.5000, 0.5000]), std=tensor([0.5000, 0.5000, 0.5000]))\n",
      ") \n",
      "\n",
      "Compose(\n",
      "    Resize(size=248, interpolation=bicubic, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    MaybeToTensor()\n",
      "    Normalize(mean=tensor([0.5000, 0.5000, 0.5000]), std=tensor([0.5000, 0.5000, 0.5000]))\n",
      ") \n",
      "\n",
      "Compose(\n",
      "    Resize(size=248, interpolation=bicubic, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    MaybeToTensor()\n",
      "    Normalize(mean=tensor([0.5000, 0.5000, 0.5000]), std=tensor([0.5000, 0.5000, 0.5000]))\n",
      ") \n",
      "\n",
      "ViT original head\n",
      "Compose(\n",
      "    Resize(size=248, interpolation=bicubic, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    MaybeToTensor()\n",
      "    Normalize(mean=tensor([0.5000, 0.5000, 0.5000]), std=tensor([0.5000, 0.5000, 0.5000]))\n",
      ") \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for cp_path, model_cfg in BEST_CHECKPOINT:\n",
    "    model,model_transforms = load_vit_model(model_cfg, NUM_CLASSES)\n",
    "    print(f\"{model_transforms} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98ca9b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=248, interpolation=bicubic, max_size=None, antialias=True)\n",
       "    CenterCrop(size=(224, 224))\n",
       "    MaybeToTensor()\n",
       "    Normalize(mean=tensor([0.5000, 0.5000, 0.5000]), std=tensor([0.5000, 0.5000, 0.5000]))\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test, model_transforms = load_vit_model(\"timm.vit_base_patch16_224.augreg_in21k.PRETRAINED\", NUM_CLASSES)\n",
    "\n",
    "CUSTOM_TRANSFORMS = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    model_transforms\n",
    "])\n",
    "model_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "281c8566",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, test_loader = create_dataloaders(\n",
    "    dataset_dir=DATASET_DIR,\n",
    "    transform=CUSTOM_TRANSFORMS,\n",
    "    batch_size=BATCH_SIZE_16,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    train_ratio=TRAIN_RATIO,\n",
    "    val_ratio=VAL_RATIO,\n",
    "    test_ratio=TEST_RATIO,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffdf42c",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa6b773c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT original head\n"
     ]
    }
   ],
   "source": [
    "models_list = []\n",
    "for cp_path, model_cfg in BEST_CHECKPOINT:\n",
    "    model,model_transforms = load_vit_model(model_cfg, NUM_CLASSES)\n",
    "    model = load_model_checkpoint(model, cp_path, DEVICE)\n",
    "    \n",
    "    models_list.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc48ec4",
   "metadata": {},
   "source": [
    "## ViT Ensemble - Testing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d0d96f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Test Loss: 0.0187 | Ensemble Test Accuracy: 0.9739\n",
      "Ensemble sensitivity: 0.9706 | Ensemble specificity: 0.9860\n"
     ]
    }
   ],
   "source": [
    "ensemble_loss, ensemble_acc , ensemble_sensitivity, ensemble_specificity = ensemble_test_step(models_list, test_loader,loss_fn, DEVICE,LOG_CHECK)\n",
    "print(f\"Ensemble Test Loss: {ensemble_loss:.4f} | Ensemble Test Accuracy: {ensemble_acc:.4f}\")\n",
    "print(f\"Ensemble sensitivity: {ensemble_sensitivity:.4f} | Ensemble specificity: {ensemble_specificity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c4818a",
   "metadata": {},
   "source": [
    "# **CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cef1f93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_CNN = [\n",
    "    [\"/home/ma012/AlexServer/log/cnn_best/torchvision_VGG16_Weights_IMAGENET1K_V1.pth\", \"torchvision.VGG16_Weights.IMAGENET1K_V1\"],\n",
    " \n",
    "    [\"/home/ma012/AlexServer/log/cnn_best/torchvision_MobileNet_V2_Weights_IMAGENET1K_V1.pth\", \"torchvision.MobileNet_V2_Weights.IMAGENET1K_V2\"], \n",
    "\n",
    "    [\"/home/ma012/AlexServer/log/cnn_best/torchvision_GoogLeNet_Weights_IMAGENET1K_V1.pth\", \"torchvision.GoogLeNet_Weights.IMAGENET1K_V1\"], \n",
    " \n",
    "    [\"/home/ma012/AlexServer/log/cnn_best/torchvision_ResNet50_Weights_IMAGENET1K_V1.pth\",  \"torchvision.ResNet50_Weights.IMAGENET1K_V2\" ],\n",
    "    \n",
    "    # [\"/home/ma012/AlexServer/log/cnn_best/torchvision_ResNet50_Weights_IMAGENET1K_V1.pth\",  \"torchvision.ResNet50_Weights.IMAGENET1K_V2\" ],\n",
    "\n",
    "    # [\"/home/ma012/AlexServer/log/cnn_best/torchvision_ResNet50_Wevights_IMAGENET1K_V1.pth\",  \"torchvision.ResNet50_Weights.IMAGENET1K_V2\" ],\n",
    "\n",
    "    # [\"/home/ma012/AlexServer/log/cnn_best/torchvision_ResNet50_Weights_IMAGENET1K_V1.pth\",  \"torchvision.ResNet50_Weights.IMAGENET1K_V2\" ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00b641e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageClassification(\n",
      "    crop_size=[224]\n",
      "    resize_size=[256]\n",
      "    mean=[0.485, 0.456, 0.406]\n",
      "    std=[0.229, 0.224, 0.225]\n",
      "    interpolation=InterpolationMode.BILINEAR\n",
      ")\n",
      "\n",
      "ImageClassification(\n",
      "    crop_size=[224]\n",
      "    resize_size=[232]\n",
      "    mean=[0.485, 0.456, 0.406]\n",
      "    std=[0.229, 0.224, 0.225]\n",
      "    interpolation=InterpolationMode.BILINEAR\n",
      ")\n",
      "\n",
      "ImageClassification(\n",
      "    crop_size=[224]\n",
      "    resize_size=[256]\n",
      "    mean=[0.485, 0.456, 0.406]\n",
      "    std=[0.229, 0.224, 0.225]\n",
      "    interpolation=InterpolationMode.BILINEAR\n",
      ")\n",
      "\n",
      "ImageClassification(\n",
      "    crop_size=[224]\n",
      "    resize_size=[232]\n",
      "    mean=[0.485, 0.456, 0.406]\n",
      "    std=[0.229, 0.224, 0.225]\n",
      "    interpolation=InterpolationMode.BILINEAR\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn_models_list = []\n",
    "\n",
    "for cp_path, model_cfg in CHECKPOINT_CNN:\n",
    "    # print(model_cfg)\n",
    "    model_cnn,model_transforms_cnn = load_cnn_model(DEVICE,model_cfg, NUM_CLASSES)\n",
    "    print(f\"{model_transforms_cnn}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c44798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "DEFAULT_SIZE = (256, 256)\n",
    "CROP_SIZE = 224\n",
    "\n",
    "DEFAULT_NORMALIZE = transforms.Normalize(mean=MEAN, std=STD)\n",
    "\n",
    "DEFAULT_TRANSFORMS = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize(DEFAULT_SIZE, interpolation=InterpolationMode.BILINEAR),\n",
    "    transforms.CenterCrop(CROP_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    DEFAULT_NORMALIZE\n",
    "])\n",
    "\n",
    "_, _, test_loader = create_dataloaders(\n",
    "    dataset_dir=DATASET_DIR,\n",
    "    transform=DEFAULT_TRANSFORMS,\n",
    "    batch_size=BATCH_SIZE_16,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    train_ratio=TRAIN_RATIO,\n",
    "    val_ratio=VAL_RATIO,\n",
    "    test_ratio=TEST_RATIO,\n",
    "    seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0266a78d",
   "metadata": {},
   "source": [
    "## **Individual Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a933429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = ['VGG16','MobileNet_V2', 'GoogLeNet', 'ResNet50', 'ConvNext-Large', 'DenseNet121', 'EfficienNet_B0']\n",
    "model_name = ['VGG16','MobileNet_V2', 'GoogLeNet', 'ResNet50']\n",
    "\n",
    "\n",
    "for idx, (cp_path, model_cfg) in enumerate(CHECKPOINT_CNN):\n",
    "    with open(LOG_CHECK, \"a\", encoding=\"utf-8\") as file:\n",
    "        file.write(f\"\\n======================   {model_name[idx]}  ==============================\\n\")\n",
    "\n",
    "    model_cnn,model_transforms_cnn = load_cnn_model(DEVICE,model_cfg, NUM_CLASSES)\n",
    " \n",
    "    model_cnn = load_model_checkpoint(model_cnn, cp_path, DEVICE)\n",
    "\n",
    "    test_cnn_results = test_step(model_cnn, test_loader, loss_fn, DEVICE,LOG_CHECK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1831c7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\"/home/ma012/AlexServer/log/cnn_best/torchvision_VGG16_Weights_IMAGENET1K_V1.pth\", \"torchvision.VGG16_Weights.IMAGENET1K_V1\"],\n",
    "\n",
    "[\"/home/ma012/AlexServer/log/cnn_best/torchvision_MobileNet_V2_Weights_IMAGENET1K_V1.pth\", \"torchvision.MobileNet_V2_Weights.IMAGENET1K_V2\"], \n",
    "\n",
    "[\"/home/ma012/AlexServer/log/cnn_best/torchvision_GoogLeNet_Weights_IMAGENET1K_V1.pth\", \"torchvision.GoogLeNet_Weights.IMAGENET1K_V1\"], \n",
    "\n",
    "[\"/home/ma012/AlexServer/log/cnn_best/torchvision_ResNet50_Weights_IMAGENET1K_V1.pth\",  \"torchvision.ResNet50_Weights.IMAGENET1K_V2\" ],\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa2c68c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model,model_transforms = load_cnn_model(\"torchvision.MobileNet_V2_Weights.IMAGENET1K_V2\", NUM_CLASSES)\n",
    "model = load_model_checkpoint(model, \"/home/ma012/AlexServer/log/cnn_best/torchvision_MobileNet_V2_Weights_IMAGENET1K_V1.pth\", DEVICE)\n",
    "\n",
    "test_results = test_step(model, test_loader, loss_fn, DEVICE,LOG_CHECK)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8477360",
   "metadata": {},
   "source": [
    "## **Ensemble Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7717a273",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_models_list = []\n",
    "\n",
    "for cp_path, model_cfg in CHECKPOINT_CNN:\n",
    "    # print(model_cfg)\n",
    "    model_cnn,model_transforms_cnn = load_cnn_model(DEVICE,model_cfg, NUM_CLASSES)\n",
    "    model_cnn = load_model_checkpoint(model_cnn, cp_path, DEVICE)\n",
    "    \n",
    "    cnn_models_list.append(model_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3790068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble_cnn Test Loss: 0.0368 | Ensemble_cnn Test Accuracy: 0.9772\n",
      "Ensemble_cnn sensitivity: 1.0000 | Ensemble_cnn specificity: 0.9930\n"
     ]
    }
   ],
   "source": [
    "ensemble_cnn_loss, ensemble_cnn_acc , ensemble_cnn_sensitivity, ensemble_cnn_specificity = ensemble_test_step(cnn_models_list, test_loader,loss_fn, DEVICE,LOG_CHECK)\n",
    "print(f\"Ensemble_cnn Test Loss: {ensemble_cnn_loss:.4f} | Ensemble_cnn Test Accuracy: {ensemble_cnn_acc:.4f}\")\n",
    "print(f\"Ensemble_cnn sensitivity: {ensemble_cnn_sensitivity:.4f} | Ensemble_cnn specificity: {ensemble_cnn_specificity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c2ce7b",
   "metadata": {},
   "source": [
    "# **Inference Stage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d53868",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES = ['glioma_tumor', 'meningioma_tumor', 'pituitary_tumor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531ea656",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\"/home/ma012/AlexServer/log/vit_best/B16_21K_case3.pth\", \"timm.vit_base_patch16_224.augreg_in21k.PRETRAINED\"],\n",
    " \n",
    "[\"/home/ma012/AlexServer/log/vit_best/B32_21K_case2.pth\", \"timm.vit_base_patch32_224.augreg_in21k.PRETRAINED\"], \n",
    "\n",
    "[\"/home/ma012/AlexServer/log/vit_best/L16_21K_case3.pth\", \"timm.vit_large_patch16_224.augreg_in21k.PRETRAINED\"], \n",
    " \n",
    "[\"/home/ma012/AlexServer/log/vit_best/L32_21K_case1.pth\",  \"timm.vit_large_patch32_224.orig_in21k.PRETRAINED\" ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94dc54aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_model, vit_model_transforms = load_vit_model(\"timm.vit_base_patch16_224.augreg_in21k.PRETRAINED\", NUM_CLASSES)\n",
    "load_vit_model = load_model_checkpoint(vit_model, \"/home/ma012/AlexServer/log/vit_best/B16_21K_case3.pth\", DEVICE)\n",
    "\n",
    "VIT_CUSTOM_TRANSFORMS = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    vit_model_transforms\n",
    "])\n",
    "\n",
    "cnn_model, cnn_model_transforms = load_cnn_model(\"torchvision.MobileNet_V2_Weights.IMAGENET1K_V2\", NUM_CLASSES)\n",
    "load_cnn_model = load_model_checkpoint(cnn_model, \"/home/ma012/AlexServer/log/cnn_best/torchvision_MobileNet_V2_Weights_IMAGENET1K_V1.pth\", DEVICE)\n",
    "\n",
    "CNN_CUSTOM_TRANSFORMS = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    cnn_model_transforms\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e8e0ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the image\n",
    "img_path = \"/home/ma012/AlexServer/Dataset/pituitary_tumor/994.jpg\"\n",
    "# image = Image.open(img_path).convert(\"L\")  # grayscale image assumed\n",
    "image = Image.open(img_path)\n",
    "\n",
    "# Apply the same transforms as trainingcnn_image_tensor\n",
    "vit_image_tensor = VIT_CUSTOM_TRANSFORMS(image).unsqueeze(0)  # shape: [1, C, H, W]\n",
    "cnn_image_tensor = CNN_CUSTOM_TRANSFORMS(image).unsqueeze(0)  # shape: [1, C, H, W]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ae52ee",
   "metadata": {},
   "source": [
    "### individual ViT and CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45cfd1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_class: 2 | softmax: tensor([0.0642, 0.0174, 0.9184], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "vit_predicted_class, vit_softmax = inference_step(load_vit_model,vit_image_tensor,DEVICE)\n",
    "\n",
    "print(f\"predicted_class: {vit_predicted_class} | softmax: {vit_softmax}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e60b0ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_class: 0 | softmax: tensor([0.5875, 0.4076, 0.0049], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "cnn_predicted_class, cnn_softmax = inference_step(load_cnn_model,cnn_image_tensor,DEVICE)\n",
    "\n",
    "print(f\"predicted_class: {cnn_predicted_class} | softmax: {cnn_softmax}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f3a47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_CHECKPOINT_CNN = [\n",
    "    [\"/home/ma012/AlexServer/log/cnn_best/torchvision_VGG16_Weights_IMAGENET1K_V1.pth\", \"torchvision.VGG16_Weights.IMAGENET1K_V1\"],\n",
    " \n",
    "    [\"/home/ma012/AlexServer/log/cnn_best/torchvision_MobileNet_V2_Weights_IMAGENET1K_V1.pth\", \"torchvision.MobileNet_V2_Weights.IMAGENET1K_V2\"], \n",
    "\n",
    "    [\"/home/ma012/AlexServer/log/cnn_best/torchvision_GoogLeNet_Weights_IMAGENET1K_V1.pth\", \"torchvision.GoogLeNet_Weights.IMAGENET1K_V1\"], \n",
    " \n",
    "    [\"/home/ma012/AlexServer/log/cnn_best/torchvision_ResNet50_Weights_IMAGENET1K_V1.pth\",  \"torchvision.ResNet50_Weights.IMAGENET1K_V2\" ],\n",
    "]\n",
    "\n",
    "cnn_models_list = []\n",
    "\n",
    "for cp_path, model_cfg in BEST_CHECKPOINT_CNN:\n",
    "    # print(model_cfg)\n",
    "    model_cnn,model_transforms_cnn = load_cnn_model(model_cfg, NUM_CLASSES)\n",
    "    model_cnn = load_model_checkpoint(model_cnn, cp_path, DEVICE)\n",
    "    \n",
    "    cnn_models_list.append(model_cnn)\n",
    "\n",
    "    CNN_CUSTOM_TRANSFORMS = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    model_transforms_cnn])\n",
    "\n",
    "    cnn_image_tensor = CNN_CUSTOM_TRANSFORMS(image).unsqueeze(0)  # shape: [1, C, H, W]\n",
    "\n",
    "for model in cnn_models_list:\n",
    "    cnn_predicted_class, cnn_softmax = inference_step(model,cnn_image_tensor,DEVICE)\n",
    "    print(f\"predicted_class: {cnn_predicted_class} | softmax: {cnn_softmax}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65a4564",
   "metadata": {},
   "source": [
    "## ViT ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d3d3c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_CHECKPOINT = [\n",
    "    [\"/home/ma012/AlexServer/log/vit_best/B16_21K_case3.pth\", \"timm.vit_base_patch16_224.augreg_in21k.PRETRAINED\"],\n",
    " \n",
    "    [\"/home/ma012/AlexServer/log/vit_best/B32_21K_case2.pth\", \"timm.vit_base_patch32_224.augreg_in21k.PRETRAINED\"], \n",
    "\n",
    "    [\"/home/ma012/AlexServer/log/vit_best/L16_21K_case3.pth\", \"timm.vit_large_patch16_224.augreg_in21k.PRETRAINED\"], \n",
    " \n",
    "    [\"/home/ma012/AlexServer/log/vit_best/L32_21K_case1.pth\",  \"timm.vit_large_patch32_224.orig_in21k.PRETRAINED\" ] \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "576bb7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT original head\n"
     ]
    }
   ],
   "source": [
    "models_list = []\n",
    "for cp_path, model_cfg in BEST_CHECKPOINT:\n",
    "    model,model_transforms = load_vit_model(model_cfg, NUM_CLASSES)\n",
    "    model = load_model_checkpoint(model, cp_path, DEVICE)\n",
    "    \n",
    "    models_list.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed53f6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class, ensemble_softmax = ensemble_inference_step(models_list,image_tensor,DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92cd6cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_class: 0 | softmax: tensor([9.9996e-01, 2.5323e-05, 1.8574e-05], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(f\"predicted_class: {predicted_class} | softmax: {ensemble_softmax}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248501c9",
   "metadata": {},
   "source": [
    "## CNN ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5e22756",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_CHECKPOINT_CNN = [\n",
    "    [\"/home/ma012/AlexServer/log/cnn_best/torchvision_VGG16_Weights_IMAGENET1K_V1.pth\", \"torchvision.VGG16_Weights.IMAGENET1K_V1\"],\n",
    " \n",
    "    [\"/home/ma012/AlexServer/log/cnn_best/torchvision_MobileNet_V2_Weights_IMAGENET1K_V1.pth\", \"torchvision.MobileNet_V2_Weights.IMAGENET1K_V2\"], \n",
    "\n",
    "    [\"/home/ma012/AlexServer/log/cnn_best/torchvision_GoogLeNet_Weights_IMAGENET1K_V1.pth\", \"torchvision.GoogLeNet_Weights.IMAGENET1K_V1\"], \n",
    " \n",
    "    [\"/home/ma012/AlexServer/log/cnn_best/torchvision_ResNet50_Weights_IMAGENET1K_V1.pth\",  \"torchvision.ResNet50_Weights.IMAGENET1K_V2\" ],\n",
    "    \n",
    "    # [\"/home/ma012/AlexServer/log/cnn_best/torchvision_ResNet50_Weights_IMAGENET1K_V1.pth\",  \"torchvision.ResNet50_Weights.IMAGENET1K_V2\" ],\n",
    "\n",
    "    # [\"/home/ma012/AlexServer/log/cnn_best/torchvision_ResNet50_Wevights_IMAGENET1K_V1.pth\",  \"torchvision.ResNet50_Weights.IMAGENET1K_V2\" ],\n",
    "\n",
    "    # [\"/home/ma012/AlexServer/log/cnn_best/torchvision_ResNet50_Weights_IMAGENET1K_V1.pth\",  \"torchvision.ResNet50_Weights.IMAGENET1K_V2\" ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6e1ef73",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_models_list = []\n",
    "\n",
    "for cp_path, model_cfg in BEST_CHECKPOINT_CNN:\n",
    "    # print(model_cfg)\n",
    "    model_cnn,model_transforms_cnn = load_cnn_model(model_cfg, NUM_CLASSES)\n",
    "    model_cnn = load_model_checkpoint(model_cnn, cp_path, DEVICE)\n",
    "    \n",
    "    cnn_models_list.append(model_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24a85d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_predicted_class, cnn_ensemble_softmax = ensemble_inference_step(cnn_models_list,image_tensor,DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29d75f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn predicted_class: 0 | softmax: tensor([0.7466, 0.0103, 0.2431], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(f\"cnn predicted_class: {cnn_predicted_class} | softmax: {cnn_ensemble_softmax}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01b1bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_labels = torch.tensor([1, 0, 2, 3])\n",
    "y = torch.tensor([1, 1, 2, 0])\n",
    "\n",
    "mismatched = test_pred_labels != y\n",
    "# tensor([False, True, False, True])\n",
    "wrong_indices = torch.nonzero(mismatched).squeeze()\n",
    "wrong_indices\n",
    "\n",
    "\n",
    "\n",
    "wrong_paths = [path[i] for i in torch.nonzero(mismatched).squeeze()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "btc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
